
网关+软防火墙(waf)+单点登录+整合缓存系统+输出动态页面
Kong lua+nginx
能扛得起亿级流量的网关 Openresty
Kong 采用openresty+lua+nginx
可以在网关中 做 路由， 软防火墙 单点登录+整合缓存系统+输出动态页面 ， 
	lua 操作redis(累加点击次数等)，异步调用kafka ，防刷、恶意流量、

	经过nginx的操作是短的，过程性的，业务简单 。

java后端 分布式事务、一致性、缓存、服务的高可用、服务管理和监控


多级缓存： 尽可能的少 的请求到tomcat ，tomcat尽可能少的请求到mysql

redis可以加载模块，nginx也可以加载模块

lua中的脚本 要尽量简洁 ， 尽量控制在1-2ms级别，甚至更短

分布式锁、抢红包


nginx一般不做集群(数据的备份、复制)，一般只做高可用 lvs(负载均衡器)
1.性能比较高
1台nginx能响应 5w的请求

商务系统，只做update

域名-》IP 
	1.现在操作系统的hosts找
	2.在操作系统的dns缓存找，缓存中没有向dns服务器中找
	3.买域名的时候，会生成一个dns服务器，广播到所有的dns服务器。
	4.还有根服务器
域名解析过程 （https://www.cnblogs.com/xsilence/p/6035559.html）
a. 浏览器缓存，浏览器会缓存DNS记录一段时间。
	操作系统没有设定浏览器存储DNS记录的时间长短，不同的浏览器会存储各自的一个固定时间，时长为2~30分钟不等。
b. 系统缓存
	如果浏览器缓存里没有找到需要的记录，浏览器会做一个操作系统调用（windows里是gethostname），这样子，就可以获得系统缓存里的记录啦。
c. 路由器缓存
	接下来，如果还是没有找到需要的缓存，将前面的查询请求发给路由器，它一般会有自己的DNS缓存。
d. 如果还是没有，那么就去检查ISP有没有吧~
	每一个ISP（网络服务提供商），或一个大学，甚至是一个大学里的系都会有一个自己的本地域名服务器，他会在url第一次访问时缓存该域名的指向。下次再访问时，他会从缓存里把这个url曾经指向的IP调出来。
e. 递归搜索
	还是没有需要的缓存~ 就只能放大招啦~
    你的ISP的DNS服务器会从根域名开始进行递归查询。
主机向本地域名服务器的查询一般都是采用递归查询。
如果主机所询问的本地域名服务器不知道被查询的域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其根域名服务器继续发出查询请求报文(即替主机继续查询)，而不是让主机自己进行下一步查询。因此，递归查询返回的查询结果或者是所要查询的IP地址，或者是返回一个失败的响应，表示无法查询到所需的IP地址。
	


	一个域名 可以绑定 多个ip ，但需要确定返回给用户哪一个ip

	全网部 nginx 节点：但nginx上放什么是个问题？静态数据、视频、，同时如何进行数据的更新也是一个问题。 分发的策略

nginx 内存缓存

cdn上提供动态请求的镜像

第一级 缓存 做CDN，全网流量分发(静态资源)
	第一级缓存主要的作用是做全网静态资源的分发，可以将一些通用的、基本不变化的资源 通过cdn做成静态缓存，分发到各个服务，直接加载，避免频繁请求nginx或后台
	但这种缓存有一定限制。如果要对商品页面进行模板的缓存，对于少量商品数据来说，可行。可以将具体的商品页封装成文件，将一些动态性的数据做成接口或消息队列。
	将文件分发到cdn服务器，数据再单独获取或定时推送。对于大量商品来说，不可行。大量商品的话生成文件个数剧增，大量的文件如果需要同步到各个CDN服务器的话，
	时效性是个很大的问题，如果数据有变化，重新同步、分发 也会出现问题。 同时，个人信息的内容不适合作为CDN缓存的内容，一个是可能更新频繁，还有个能数据泄露

大型电商 如果做cdn 静态文件缓存的话，会很难操作
小型电商可以通过cdn 做静态资源的全网分发、缓存。可以将 


使用lua_resty_redis 连接 redis 


提高程序访问速度，数据实时一致性 要求不高，最终一致性保证即可

通过nginx+lua 将 流量定向转发，每个url请求转发到一个固定的服务器上，所以这个服务器上的数据和静态模板 可以缓存成固定数据
定时更新，但会牺牲一致性，


基于客户端的缓存
	浏览器 :
	    一次加载，之后基本不会变化的
		local storage 关闭浏览器不会丢失
		session storage 会话相关存储，关闭浏览器丢失
	client	


6个9
99.9999% 
报证 数据的可用性，数据的一致性

脑裂问题:
在一个高可用系统中，当联系着的节点断开联系时，本来为一个整体的系统，分裂成两个独立节点，两个节点开始争抢共享资源造成系统混乱、数据损坏的现象，成为“脑裂”。

分布式的锁

数据库方式: 	数据库也是 分布式的，数据的同步是一个问题，同步过程中出现问题，可能会存在多个锁的关系

redis : 主从复制，数据同步 ，AOF和RDB 持久化的数据同步  redis集群在数据同步的时候，不知道什么时候结束。
		master宕机，还有数据可能没有同步，导致数据存在差异
		redis 的强一致性没那么高，可用性比较强 ，高速访问

zookeeper ： 主要是协调 ，一致性要比redis强

分区容错性:
	分区: 单节点:未分区，多节点: 分区  跨网络  , 出了错误，可以进行容错   分区容错性无法达到100%

CP 允许网络异常的情况下，提高数据的一致性 。 不是极强的一致性，尽可能的提高一致性(可能因为网络，达不到 P 的100%)。在数据未同步的情况下，就没有办法满足 A (可用性) zookeeper

AP 高可用/分区容错性 前提 一定要满足高可用，但数据的强一致性可能就没有办法满足，无法做到 没有 数据同步的 时间窗口 erueka 

CA 高可用/强一致性  不能出现错误数据 。似乎不成立 ；// 但 满足 P 的话，可能会出现一致性问题
					

资源隔离

线程隔离:
	服务和服务之间采用多线程


	用户通过netty 长连接 接进来，把请求转发给各个系统，netty使用线程池管理网络请求，业务线程也有自己的线程池，业务线程池设置能处理请求的阈值(单一jvm能处理的业务线程数有上限)，业务线程池中，给每个业务配置响应的线程池，按照业务的使用率 进行资源的分配(线程)。
	不可以使用线程池的原因。真正计算的还是系统的cpu和所占的内存，如果某个业务的线程占用cpu等很高，频繁GC

	tomcat的业务线程中的servlet中，存在状态信息，放在threadlocal中，做一个线程的隔离 (servlet 2.0 tomcat <6) 最大的问题是 请求的接受和处理都是一起的

请求放到kafka里，如何返回给用户
	nginx+lua+kafka+后台服务

如何去做服务限流的功能

	首先可以建立多少连接，连接建立成功之后在考虑业务可以处理多少连接 
	创建连接的时候，可以使用计数器算法  

	nginx 设置限流: 配置limit_conn_zone
	http {
		limit_conn_zone $binary_remote_addr zone=addr;10m;
		limit_conn_status 503;
		limit_conn_log_level error;
		server {
			location /limit {
				limit_conn addr 1; /nginx同时能处理的并发数
			}
		}

	}

	漏桶算法的缺点:每一个处理的业务线都极其饱和，cpu始终是100%，还有时间窗口的问题
	令牌桶算法:主要体现的是响应连接方面。 多长时间拿一次令牌  
	在 固定的时间窗口内，处理多少链接,每次都从令牌捅中取令牌，拿到令牌才能处理请求，没有令牌就阻塞，重新拿到令牌在接着处理

服务降级:
	一部分功能组件出席那一场，不能让所有服务都失败，
	SLA 服务等级协议 ： 服务商与您达成的正常运行时间保证

	降级处理:
		兜底数据: (服务出问题的时候，给一个友好的反馈方式)可以设置默认值、、返回一些静态数据、缓存
			汇源  回到数据的源头

		没写进去，还用之前的老数据(需要一致性要求没那么高的情况)
		超时降级：有一个可容忍的时间，如果在超时时间内，请求多少次之后依然服务不可用，才判定为服务超时
			判断服务降级的小工具: 可以采用一个探测器，定时的探测每个服务的可用性，如果不可用标记为一个状态，可用，标记为另一个状态。
				当其他服务访问这个服务的时候，先判定这个服务是否可用，如果不可用直接通知用户不可用

				降级开关可以采用redis或者zookeeper

		幂等性: 相同的消息，只处理一次

		写降级:
			先写数据库，再写缓存。 分布式事务

			不管是先写数据库还是先写缓存，对数据库的更新都是直接写数据库，对数据库的压力太大
			引入消息队列 先写入redis ，再写入kafka ，交由其他程序写入到 数据库
			从kafka到数据库中的时候，如果其中不加任何内容，还是直接不断的写，可以做个限流，对数据库的压力减小

分布式事务

 弱一致性可以换取高可用性

 两阶段提交(提供强一致性): 事务发起者将一系列操作 发送给 一个中间人，该中间人的主要作用是 记录当前的事务 要操作那些表的数据(该操作可以对其他的事务进行一个隔离的作用)
 						 最主要的作用还是对要操作的连接发送一个创建请求，如果请求成功，则返回true，后续的操作向下接着看能不能创建请求，如果都可以创建，那么第一阶段通过
 						 第二阶段可以直接提交
 三阶段提交: 对两阶段做了一个补充
 			预备阶段(检查网络) -》 准备阶段(预写入) -》 提交(持久化)

 TCC(手动实现两阶段的方案) ：  
 		在tcc 的上层，会有一个主线业务，当一个事务开启的时候，会对该事务 创建一个 唯一 的事务ID ，可以做 confirm阶段的幂等、回滚等操作
 		try ： 尝试，预准备阶段 (检查网络连接，检查所有业务，隔离资源 )
 		confirm  补偿(不连接一次就返回，可以限定尝试的次数) 幂等()
 		cancel

XA模式解析
	https://www.jianshu.com/p/044e95223a17
	seata分布式执行流程
		1.TM(Transaction Manager)向 TC(Transaction Coordinator)申请全局事务，全局事务创建成功，并行成一个全局唯一的XID
		2.XID在微服务调用链中传播
		3.RM(Resource Manager)开始执行这个分支事务,解析这条语句，生成对应的undo_log(存在具体RM的本地，而不是mysql的undo_log)
		4.RM在同一个本地事务中执行业务SQL和UNDO_LOG数据的插入，但需要向TC申请关于这条记录的全局锁。如果申请不到，则说明有其他事务也在对这条记录进行操作，因此它会在一段时间内重试，重试失败则回滚本地事务，并向TC汇报本地事务执行失败。
		5.RM提交事务前，申请到了相关全局锁，提交事务，并向TC汇报本地事务执行成功，但不会释放相关的全局锁
		6.TC根据所有分支事务的执行结果，向RM下发提交或回滚命令
		7.RM收到tc的提交命令，释放全局锁，提交请求放入异步队列，立即返回提交成功给TC，异步队列中的请求真正执行时，只是删除undo_log记录
		8.如果回滚命令，开启一个本地事务，通过XID和BRanchID 找到对应的undo_log ，将undo_log中的后经和当前数据比较，如果不同，说明数据被全局事务之外的动作修改了，根据配置策略处理，否则，根据undo_log中的经前进行镜像和业务相关信息生成并执行回滚的语句并指向，提交本地事务达到回滚的目的，释放相关的全局锁

XA两阶段提交

TCC方案


SAGA方案:
	采用saga作为长事务的解决方案
	对于一致性要求高、短流程、并发高 的场景，如：金融核心系统，会优先考虑 TCC 方案
	所以 Saga 模式的适用场景是：
		业务流程长、业务流程多；
		参与者包含其它公司或遗留系统服务，无法提供 TCC 模式要求的三个接口。
本地消息表

可靠消息最终一致性方案
	使用带有 事务消息队列，RocketMq ，保证消息的一致性，通过消息的不同状态进行事务的提交

最大努力通知方案



微服务的拆分：
	问题点:
		拆成多大合适？
		多个服务能否共享数据库？
		每个服务数据库独立，后台管理的联合查询如何处理？
	设计思路:
		业务相关的表放到一个库中，业务无关的表严格按照微服务模式拆分；
		严格按照微服务切分，为满足高并发，实时或者准实时将服务数据同步到nosql中，同步过程进行数据清洗，以满足后台业务系统

数据库-缓存一致性的保证:
	一般遵循 Cache Aside Pattern
		读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据放入缓存，同时返回响应
		更新的时候，先更新数据库，在删除缓存。
	第一个问题: 为什么删除缓存，而不是更新缓存
		在复杂的场景下，缓存的值不单单是直接取出来的，可能是根据几个中间结果计算出来的。
		同时，如果针对某个数据的数据库更新很频繁，但对该数据的查询频率很低，那么也不应该更新缓存。频繁更新会导致计算次数多，间接影响效率
	第二个问题: 先更新数据库，在删除缓存，如果缓存删除失败了，数据库终就是新数据，缓存终就是旧数据，数据还是不一致
		该问题的解决方案为 先删除缓存，再更新数据库	。删除缓存之后，如果更新数据库失败，那么缓存也是空数据，重新读取的时候，还是读取的数据库的旧数据
	第三个问题: 高并发的情况下，A线程 先删除缓存，再更新数据库，B线程在A删除缓存之后，立即进行了查询，缓存中就是旧数据了，最后A线程更新成功。
		高并发情况下这种情况出现的概率会更高。解决该问题的方案可以采用下面的方法:
			更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新执行“读取数据+更新缓存”的操作，根据唯一标识路由之后，也发送到同一个 jvm 内部队列中。

			一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。

			这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。


网关 做接入链接，分析链接
	流量网关 
		全局性的控制流量的接入 (控制阈值，理论上这个阈值不允许被达到)
		日志统计 直接在此处记录请求的日志，不走后续的请求
		sql注入
		防止web攻击(防刷，上传限制等)
	openresty+waf做防火墙 waf 网址 https://github.com/unixhot/waf

	业务网关  接入用户的请求
	kong的缺点: 基于代理模式的，从请求开始，到请求结束会经过一个较长的路由，不适合做大流量(文件上传等)功能，需要做请求的隔离，将大流量的请求转发到其他的资源上去

	zuul，gateway 微服务内部做请求的转发 (服务对服务，而不是直接面向用户的流量)

	1.提供了路由的服务

实时性高:各个微服务feign调用 ，多库的join操作，取联合数据

最终一致性:	中央缓存数据，冗余存储(通过job拿数据)
页面详情页的解决方案
中小型方案
1.图片数据 CDN缓存，引入cdn，加速图片的加载
2.数据的统一入口 item.html?id=123  通过nginx对id做hash，进行定向的流量分发不同的id请求打到不同的服务器上去
3.数据的聚合 ajax异步，服务拆分 ，请求统一的数据聚合，可以将后台的不同数据(图文数据)聚合到 数据缓存中(但要控制每条数据的大小，最好不要超过500k)，
4.低响应延迟，高可用    第一时间返回整体页面的骨架，然后内部数据 符合当代浏览器的加载规则。服务器端可以直接返回一个拼装好的html，这样客户端就不需要根据数据渲染html，直接加载
5.相同的数据，不同结构去展示  使用模板去做，生成不同的模板，可以放在nginx+lua中
模板引擎:	thymeleaf , freemarker , JFinal

	nginx ssi 将多个文件合并成一个 (生成一个页面的时候，可以将头、尾、中间内容分别生成一个文件，最终通过ssi将三个文件合并成一个文件。
	这样如果头、尾发生变化的时候，只需要更改对应的文件就好。避免了生成整个html需要全部重新生成的问题，可以针对中小型电商使用)

	静态文件的问题
		文件多:

		碎片化文件多
		随机小文件读写

		使用 tmpfs 。使用内存存储(利用内存空间)(小型电商)

		如果文件再多，使用负载均衡的方式，生成文件之后，只更新一台服务器，服务器集群内部自己做同步
		remotesync(linux的服务软件，支持文件增量修改，文件少的时候可用，中小型)

静态化文件生成业务流程及技术选型解决方案

redis读写机制:
	单线程 大 value 在 单线程下产生的问题: 线程阻塞，产生在往外面write的时候
										让其他缓存失效，直接造成缓存穿透(200k在redis中已经算是个很大的容量，尽量控制在10k的value，set或者map要在5000个以内)
										设计到 操作系统的 epoll 模式，然后是拿到数据之后，还会给网卡打回数据


大型方案:
	openresty+lua

openssl req -subj "/CN=39.103.205.254" -sha256 -new -key server-key.pem -out server.csr

echo subjectAltName = IP:192.168.20.22,IP:0.0.0.0 >> extfile.cnf

