
网关+软防火墙(waf)+单点登录+整合缓存系统+输出动态页面
Kong lua+nginx
能扛得起亿级流量的网关 Openresty
Kong 采用openresty+lua+nginx
可以在网关中 做 路由， 软防火墙 单点登录+整合缓存系统+输出动态页面 ， 
	lua 操作redis(累加点击次数等)，异步调用kafka ，防刷、恶意流量、

	经过nginx的操作是短的，过程性的，业务简单 。

java后端 分布式事务、一致性、缓存、服务的高可用、服务管理和监控


多级缓存： 尽可能的少 的请求到tomcat ，tomcat尽可能少的请求到mysql

redis可以加载模块，nginx也可以加载模块

lua中的脚本 要尽量简洁 ， 尽量控制在1-2ms级别，甚至更短

分布式锁、抢红包


nginx一般不做集群(数据的备份、复制)，一般只做高可用 lvs(负载均衡器)
1.性能比较高
1台nginx能响应 5w的请求

商务系统，只做update

域名-》IP 
	1.现在操作系统的hosts找
	2.在操作系统的dns缓存找，缓存中没有向dns服务器中找
	3.买域名的时候，会生成一个dns服务器，广播到所有的dns服务器。
	4.还有根服务器

	一个域名 可以绑定 多个ip ，但需要确定返回给用户哪一个ip

	全网部 nginx 节点：但nginx上放什么是个问题？静态数据、视频、，同时如何进行数据的更新也是一个问题。 分发的策略

nginx 内存缓存

cdn上提供动态请求的镜像

第一级 缓存 做CDN，全网流量分发(静态资源)
	第一级缓存主要的作用是做全网静态资源的分发，可以将一些通用的、基本不变化的资源 通过cdn做成静态缓存，分发到各个服务，直接加载，避免频繁请求nginx或后台
	但这种缓存有一定限制。如果要对商品页面进行模板的缓存，对于少量商品数据来说，可行。可以将具体的商品页封装成文件，将一些动态性的数据做成接口或消息队列。
	将文件分发到cdn服务器，数据再单独获取或定时推送。对于大量商品来说，不可行。大量商品的话生成文件个数剧增，大量的文件如果需要同步到各个CDN服务器的话，
	时效性是个很大的问题，如果数据有变化，重新同步、分发 也会出现问题。 同时，个人信息的内容不适合作为CDN缓存的内容，一个是可能更新频繁，还有个能数据泄露

大型电商 如果做cdn 静态文件缓存的话，会很难操作
小型电商可以通过cdn 做静态资源的全网分发、缓存。可以将 


使用lua_resty_redis 连接 redis 


提高程序访问速度，数据实时一致性 要求不高，最终一致性保证即可

通过nginx+lua 将 流量定向转发，每个url请求转发到一个固定的服务器上，所以这个服务器上的数据和静态模板 可以缓存成固定数据
定时更新，但会牺牲一致性，


基于客户端的缓存
	浏览器 :
	    一次加载，之后基本不会变化的
		local storage 关闭浏览器不会丢失
		session storage 会话相关存储，关闭浏览器丢失
	client	


6个9
99.9999% 
报证 数据的可用性，数据的一致性

脑裂问题:
在一个高可用系统中，当联系着的节点断开联系时，本来为一个整体的系统，分裂成两个独立节点，两个节点开始争抢共享资源造成系统混乱、数据损坏的现象，成为“脑裂”。

分布式的锁

数据库方式: 	数据库也是 分布式的，数据的同步是一个问题，同步过程中出现问题，可能会存在多个锁的关系

redis : 主从复制，数据同步 ，AOF和RDB 持久化的数据同步  redis集群在数据同步的时候，不知道什么时候结束。
		master宕机，还有数据可能没有同步，导致数据存在差异
		redis 的强一致性没那么高，可用性比较强 ，高速访问

zookeeper ： 主要是协调 ，一致性要比redis强

分区容错性:
	分区: 单节点:未分区，多节点: 分区  跨网络  , 出了错误，可以进行容错   分区容错性无法达到100%

CP 允许网络异常的情况下，提高数据的一致性 。 不是极强的一致性，尽可能的提高一致性(可能因为网络，达不到 P 的100%)。在数据未同步的情况下，就没有办法满足 A (可用性) zookeeper

AP 高可用/分区容错性 前提 一定要满足高可用，但数据的强一致性可能就没有办法满足，无法做到 没有 数据同步的 时间窗口 erueka 

CA 高可用/强一致性  不能出现错误数据 。似乎不成立 ；// 但 满足 P 的话，可能会出现一致性问题
					

资源隔离

线程隔离:
	服务和服务之间采用多线程


	用户通过netty 长连接 接进来，把请求转发给各个系统，netty使用线程池管理网络请求，业务线程也有自己的线程池，业务线程池设置能处理请求的阈值(单一jvm能处理的业务线程数有上限)，业务线程池中，给每个业务配置响应的线程池，按照业务的使用率 进行资源的分配(线程)。
	不可以使用线程池的原因。真正计算的还是系统的cpu和所占的内存，如果某个业务的线程占用cpu等很高，频繁GC

	tomcat的业务线程中的servlet中，存在状态信息，放在threadlocal中，做一个线程的隔离 (servlet 2.0 tomcat <6) 最大的问题是 请求的接受和处理都是一起的

请求放到kafka里，如何返回给用户
	nginx+lua+kafka+后台服务

如何去做服务限流的功能

	首先可以建立多少连接，连接建立成功之后在考虑业务可以处理多少连接 
	创建连接的时候，可以使用计数器算法  

	nginx 设置限流: 配置limit_conn_zone
	http {
		limit_conn_zone $binary_remote_addr zone=addr;10m;
		limit_conn_status 503;
		limit_conn_log_level error;
		server {
			location /limit {
				limit_conn addr 1; /nginx同时能处理的并发数
			}
		}

	}

	漏桶算法的缺点:每一个处理的业务线都极其饱和，cpu始终是100%，还有时间窗口的问题
	令牌桶算法:主要体现的是响应连接方面。 多长时间拿一次令牌  
	在 固定的时间窗口内，处理多少链接,每次都从令牌捅中取令牌，拿到令牌才能处理请求，没有令牌就阻塞，重新拿到令牌在接着处理

服务降级:
	一部分功能组件出席那一场，不能让所有服务都失败，
	SLA 服务等级协议 ： 服务商与您达成的正常运行时间保证

	降级处理:
		兜底数据: (服务出问题的时候，给一个友好的反馈方式)可以设置默认值、、返回一些静态数据、缓存
			汇源  回到数据的源头

		没写进去，还用之前的老数据(需要一致性要求没那么高的情况)
		超时降级：有一个可容忍的时间，如果在超时时间内，请求多少次之后依然服务不可用，才判定为服务超时
			判断服务降级的小工具: 可以采用一个探测器，定时的探测每个服务的可用性，如果不可用标记为一个状态，可用，标记为另一个状态。
				当其他服务访问这个服务的时候，先判定这个服务是否可用，如果不可用直接通知用户不可用

				降级开关可以采用redis或者zookeeper

		幂等性: 相同的消息，只处理一次

		写降级:
			先写数据库，再写缓存。 分布式事务

			不管是先写数据库还是险些缓存，怼数据库的更新都是直接写数据库，怼数据库的压力太大
			引入消息队列 先写入redis ，再写入kafka ，交由其他程序写入到 数据库
			从kafka到数据库中的时候，如果其中不加任何内容，还是直接不断的写，可以做个限流，怼数据库的压力减小

分布式事务

 弱一致性可以换取高可用性

 两阶段提交(提供强一致性): 事务发起者将一系列操作 发送给 一个中间人，该中间人的主要作用是 记录当前的事务 要操作那些表的数据(该操作可以对其他的事务进行一个隔离的作用)
 						 最主要的作用还是对要操作的连接发送一个创建请求，如果请求成功，则返回true，后续的操作向下接着看能不能创建请求，如果都可以创建，那么第一阶段通过
 						 第二阶段可以直接提交
 三阶段提交: 对两阶段做了一个补冲
 			预备阶段(检查网络) -》 准备阶段(预写入) -》 提交(持久化)

 TCC(手动实现两阶段的方案) ：  
 		在tcc 的上层，会有一个主线业务，当一个事务开启的时候，会对该事务 创建一个 唯一 的事务ID ，可以做 confirm阶段的幂等、回滚等操作
 		try ： 尝试，预准备阶段 (检查网络连接，检查所有业务，隔离资源 )
 		confirm  补偿(不连接一次就返回，可以限定尝试的次数) 幂等()
 		cancel

XA模式解析
	https://www.jianshu.com/p/044e95223a17
	seata分布式执行流程
		1.TM(Transaction Manager)向 TC(Transaction Coordinator)申请全局事务，全局事务创建成功，饼成成一个全局唯一的XID
		2.XID在微服务调用链中传播
		3.RM(Resource Manager)开始执行这个分支事务,解析这条语句，生成对应的undo_log(存在具体RM的本地，而不是mysql的undo_log)
		4.RM在同一个本地事务中致性业务SQL和UNDO_LOG数据的插入，但需要向TC申请关于这条记录的全局锁
		5.RM提交事务前，申请到了相关全局锁，提交事务，并向TC会报本地事务致性成功，但不会释放相关的全局锁
		6.TC根据所有分支事务的致性结果，向RM下发提交或回滚命令
		7.RM收到tc的提交命令，释放全局锁，提交请求放如异步队列，立即返回提交成功给TC，异步队列中的请求真正执行时，只是删除undo_log记录
		8.如果回滚命令，开启一个本地事务，通过XID和BRanchID 找到对应的undo_log ，将undo_log中的后经和当前数据比较，如果不同，说明数据被全局事务之外的动作修改了，根据配置策略处理，否则，根据undo_log中的经前进行镜像和业务相关信息生成并执行回滚的语句并指向，提交本地事务达到回滚的目的，释放相关的全局锁
微服务的拆分：
	问题点:
		拆成多大合适？
		多个服务能否共享数据库？
		每个服务数据库独立，后台管理的联合查询如何处理？
	设计思路:
		业务相关的表放到一个库中，业务无关的表严格按照微服务模式拆分；
		严格按照微服务切分，为满足高并发，实时或者准实时将服务数据同步到nosql中，同步过程进行数据清洗，以满足后台业务系统

openssl req -subj "/CN=39.103.205.254" -sha256 -new -key server-key.pem -out server.csr

echo subjectAltName = IP:192.168.20.22,IP:0.0.0.0 >> extfile.cnf

