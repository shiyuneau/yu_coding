
G:\working_space\common_project>G:\javaSoft\Java\jdk1.8.0_131\bin\javap -verbose G:\working_space\common_project\thread_coding\target\classes\coding\Add.class
没有GC
total=123.0
free=117.74468994140625
max=1796.0

GC之后
total=123.0
free=119.68526458740234
max=1796.0

加了int再GC
total=123.0
free=121.6852798461914
max=1796.0

加了int不GC
total=123.0
free=117.74468994140625
max=1796.0
useserialgc


常见面试题:
https://www.cnblogs.com/heqiyoujing/p/11146178.html

https://zhuanlan.zhihu.com/p/102500311


point1: 
es mapping验证相关(es mapping的设置一般只需要增加，而不需要更新。更新的话需要先移除数据，再重新设置mapping，然后再进行索引)
步骤如下:(通过RestHighLevelClient，可以再创建索引的时候就制定对应的mapping类型，不一定要先创建一条数据)
    1.创建一个索引
        POST mapping_test2/_doc
        {
          "field":"one"
        }
    2.配置动态配置mapping关闭
        PUT mapping_test2/_mapping
        {
          "dynamic":"false"
        }
    3.针对该索引，手动添加一个mapping
        PUT mapping_test2/_mapping
        {
          "properties":{
            "field_3":{
              "type" : "text",
                  "fields" : {
                    "keyword" : {
                      "type" : "keyword",
                      "ignore_above" : 256
                    }
                  }
            }
          }
        }
    4.再添加一条field_3的数据
        POST mapping_test2/_doc
        {
          "field_3":"three"
        }
    5.搜索field_3,可以正常搜索出结果
        GET mapping_test2/_search
        {
          "query": {
            "match": {
              "field_3": "three"
            }
          }
        }
    6.如果直接添加一条field_2的数据，那么使用match搜索，搜索不到，就算是再次添加field_2对应的mapping，也match不到对应的数据。针对这种情况只能重新索引

point2:
    5.0之前，es的字符串类型使用string ，在5.0之后采用text代替string，默认256个字符。并且新增了对应text的keyword关键字。
    实际上 text和keyword是两个类型，text关键字会进行分词，走分析的流程，支持模糊、精确查询，但不支持聚合。keyword不会进行分词，不走分析，直接索引，也可模糊、精确查询，支持聚合
    取消了index的analyzed,not_analyzed,no关键字，现在index只能使用true或者false。
    index = true，代表es会为该属性创建分词，可以当作主查询条件 (默认为true)
    index = false,代表es不会为该属性创建分词，不能当作主查询条件。  如果一个字段 的index 设置为false，那么当在该字段上进行索引查询的话会报错

    5.0之后 filtered被bool代替。 这种情况还会计算得分，只有单独的filter的时候，才不会计算得分。。(按网上的说法都是过滤器 先执行，在计算query，不会计算得分，但这里好像不是这个问题，还是会计算得分)
    原filtered写法
    GET _search
    {
      "query": {
        "filtered": {
          "query": {
            "match": {
              "text": "quick brown fox"
            }
          },
          "filter": {
            "term": {
              "status": "published"
            }
          }
        }
      }
    }
    现在 bool的写法: 
    GET _search
    {
      "query": {
        "bool": {
          "must": {
            "match": {
              "text": "quick brown fox"
            }
          },
          "filter": {
            "term": {
              "status": "published"
            }
          }
        }
      }
    }




    关于text 的keyword 。keyword代表是否进行分词。当对 字符串进行查询的时候，不加 keyword,会进行分词的查询，加上keyword 会进行不分词 的查询。如下
        1.创建一个字段
        POST mapping_test2/_doc
        {
            "field_7":"seven test"
        }   

        2.使用keyword查询，结果为null ， 去掉keyword查询，可以查出刚才插入的结果
        GET mapping_test2/_search
        {
          "query": {
            "match": {
              "field_7.keyword": "test"
            }
          }
        }

point3: 
    2.0的时候，浮点数默认的类型是double ，5.0更新后默认使用float。相对于double，可以降低存储需求。但有些情况下需要使用double，就要在mapping中手动指定double

point4:
    针对日期类型，es会解析这个字符串，然后将其作为long的整数存入lucene的索引。(从1970-1-1 utc时间，毫秒)，搜索文档时，如果提供的也是date的字符串，es也会解析为数值来处理

point5:
    es 中不存在所谓的更新操作，其更新操作都是用心的文档替换旧的文档。可以使用下面的语句进行更新(index/type/id/_update)，但也都是用新文档替换旧文档。同时还可以使用脚本(Groovy)进行更新
        POST mapping_test2/_doc/VFlNn3sB-z8hYu2zYVFZ/_update
        {
          "doc":{ 
            "field_7":"seven update"
          }
        }
    upsert代表如果更新的文档不存在的话，会初始化一个新的文档
        POST mapping_test2/_doc/VFlNn3sB-z8hYu2zYVFZ/_update
        {
          "doc":{ 
            "field_7":"seven update"
          },
          "upsert":{
            "":"",
            "":""
          }
        }

point6:
    删除索引 就是移除了 和索引分片相关的文件。和删除单独的文档相比，删除文件系统中的文件更快。
    分段:
        一个分段是建立索引的时候所创建的一块Lucene索引(即分片)。当索引新文档时，内容不会添加到分段的尾部，而只会创建新的分段。由于删除操作只是将文档标记为待删除，所以分段中的数据不会被移除。最终，更新文档意味着重新索引，数据永远不会被修改。
        当es再分片上进行查询的时候，lucene需要查询所有的分段，合并结果，然后将其返回。分片越多，分段越多，搜索请求越慢。  日常的索引操作会产生很多这样的小分段。为了避免索引中存在过多的分段，lucene定期将分段进行合并
    合并:
        读取文档的内容(除了被删除的文档)，利用组合的内容 创建新的，更大的分段。需要CPU和磁盘IO。合并操作是异步运行的。

point7:
    过滤器只是为了 文档是否匹配这个查询，返回 是 或者 否。但查询是为了计算得分。使用term查询，得分的结果都是1.0
    过滤器的查询要比普通的查询快，而且还可以被缓存

    在一个查询语句中，先使用query，然后使用filter，会进行score得分排名
    对查询的一个说明:
        GET bdms_index_462_t_sp_gas_area_discharge/_search
        {
          "query": {
            "match": {
              "cityname": "贵阳市"
            }
          }, 
          "post_filter": {
            "term": {
              "regionname": "区"
            }
          }
        }
        es 会建立一个 位集合(bitset) ，es会根据这个位集合进行过滤，根据搜索的查询部分，排除掉不应该被搜索的文档。过滤器限制了需要计算得分的文档数量，根据查询，仅仅有限的文档集合才需要计算得分。过滤器还可以用于另外的搜索请求。
        如果es可以预见过滤器不会在被使用，或者位集合重新创建的成本微乎其微，那么过滤器不会自动被缓存。

point8:
    如果指定了排序字段，那么_score字段将失效(不会按照得分进行排序)，_score就为null

point9: 各种查询的使用场景和举例
    
    更推荐的是使用过滤器查询，一个是不会计算得分，然后 如果过滤器多次被使用，还会对其进行缓存

    1.match_all 匹配所有的文档，后面可以接 term 过滤的条件，得分会被忽略，但一般不太使用的
        GET bdms_index_462_t_sp_gas_area_discharge/_search
        {
          "query": {
            "match_all": {
            }
          }, 
          "post_filter": {
            "term": {
              "regionname": "区"
            }
          }
        }
    2.query_string 通过url _search?q=test  这种就是使用的query_string的查询，该查询会查询_all字段(默认所有字段组合而成)(_all 在7.0之后被移除)。如果要修改成查询某个字段，可以使用default_field.
        同时 通过query_string ,还可以使用 and or 等操作。query_string不仅简单而且强大， 但是当查询条件多时，阅读上就会比较费力，而且直接通过url查询会导致es集群的风险太高，所以一般建议使用term，terms，match或者multi_match进行查询
    3.term和term过滤器
        单独使用term也可以进行查询，会计算得分。(但这个和match_all有啥区别呢？term 不会进行大小写的转换，大写就还是大写，但是match会进行 大写转小写)
            GET bdms_index_462_t_sp_gas_area_discharge/_search
            {
              "query": {
                "term": {
                  "regionname": {
                    "value": "白云"
                  }
                }
              }
            }
        term过滤器，使用term过滤器可以用来限制结果文档，使其包含指定的词条，而且无需计算得分(该语句中，如果不带有query不会计算得分，带有query会计算得分，如果match换成match_all，也不会计算得分(可能是因为match_all也是返回所有文档))
            GET bdms_index_462_t_sp_gas_area_discharge/_search
            {
              "query": {
                "match": {
                  "cityname": "贵阳市"
                }
              }, 
              "post_filter": {  // post_filter 查询之后过滤
                "term": {
                  "regionname": "区"
                }
              }
            }
    4.terms标签
        terms标签可以查询多个标签，如下，查询 字段中，含有 区 或者 明 的文档
        GET bdms_index_462_t_sp_gas_area_discharge/_search
        {
            "post_filter": {
            "terms": {
                "regionname": ["区","明"]
                }
            }
        }
        可以通过minimum_should_match参数来强制规定每篇文档中匹配词条的最小数量
        针对 minimum_should_match ，有匹配的一套规则
    5.match查询
        match查询是一个散列映射，包含了希望搜索的字段和字符串。match可以有多种行为方式，最常见的是boolean和词组(phrase)
        布尔查询   。 如果单独使用 match，不加布尔，那么会查询service或before只要存在一个就可以的结果。当使用operator=and时，查询的结果就是 两个词都含有的结果
            GET mapping_test3/_search
            {
              "query": {
                "match": {
                  "field_7": {
                    "query":"service before",
                    "operator": "and"
                  }
                  
                }
              }
            }
        词组查询  如果只知道一个或两个单词，可以通过词组的方式进行查询(由于英文中存在空格的行为，这个可以进行类似于模糊的查询)使用slot=1或者2(不加的话默认是0) 代表词之间是有间隔的
        GET mapping_test3/_search
        {
          "query": {
            "match_phrase": {
              "field_7": { 
                "query":"service before",
                "slop":1
              }
              
            }
          }
        }

        match和match_phrase性能对比: match query 比 match_phrase 的性能要高 10 倍，比 proximity match（带slop的）性能要高20倍;match_phrase 据说比 term 查询慢 20 倍。还是建议使用match

        词组前缀查询 : match_phrase_prefi会根据词组中最后一个词条进行前缀匹配。通过max_expansions来设置最大的前缀扩展数量。下面实例中的service7 ser可以查到，但是换成servi7 ser就不会查到了，要保证前面的词是完全匹配的才可以
        GET mapping_test3/_search
        {
          "query": {
            "match_phrase_prefix": {
              "field_7": { 
                "query":"service7 ser",
                "max_expansions":3
              }
              
            }
          }
        }

        匹配多个字段: multi_match可以用来匹配多个字段
        GET mapping_test3/_search
        {
          "query": {
            "multi_match": {
                "query":"service7 ser",
                "fields":["field_6","field_7"]
            }
          }
        }
    6.bool查询
        bool查询可以任意组合查询的数量，内部包含must(必须匹配)，should(至少包含minimum_should_match个数的条件)，must_not(必须移除，不包含)三种条件。should可以在一定条件下改成terms语句，并行写在must语句中
        每个选项的内部，可以有多个条件，继续写term，match等条件
        GET mapping_test3/_search
        {
          "query": {
            "bool": {
              "must": [
                {
                  "term": {
                    "FIELD": {
                      "value": "VALUE"
                    }
                  }
                },
                {
                  "match": {
                    "FIELD": "TEXT"
                  }
                },
                {
                  "multi_match": {
                    "query": "",
                    "fields": []
                  }
                }
              ],
              "should": [
                {},
                {}
              ],
              "must_not": [
                {}
              ],
              "minimum_should_match": 1
            }
          }
        }

        bool过滤器: 相当于使用过滤的条件下使用 bool
        GET /lib4/items/_search
        { 
          "post_filter":{
            "bool":{
              "should":[
               {"term": {"itemID.keyword": "ID100124"} },
               {"term":{"price":25}}
                ],
                "must_not":{"term":{"price":50}}
            }
          }
        }

    7.range查询 : 查询介于一定范围的值，适用于 数字、日期和字符串    ，需要指定 上届和下届值 (对于时间的查询还是有问题，入库的时间是有8小时时间间隔的，但查询如果按照正常得事件，range查询会将那部分数据省略掉)
        GET bdms_index_380_summarydata/_search
        {
          "query": {
            "range": {
              "IndicatorDate": {
                "gte": "2016-12-31",
                "lte": "2017-01-05"
              }
            }
          }
        }                                                                                                                                                                                                   

    8.wildcard查询
        类似于shell 的通配符的查询方式。* 匹配任何字符序列 ， ? 匹配一个单独字符。但这个应该也慎用，因为匹配的话，会从 通配符之前的值开始查找子集，前缀越多，子集越小，效率越高。使用的收需要考虑到额外开支和性能
        GET bdms_index_462_t_sp_gas_area_discharge/_search
        {
          "query": {
            "wildcard": {
              "regionname": {
                "value": "白?区"
              }
            }
          }
        }                                                                                                                                                                                                  

point10: 分析数据
    在文档被发送至倒排索引之前，es在其主体上进行的操作，可能包含以下步骤:
        (以 share experience with NoSql & big technologies 为例)
        input -> characterFilters(字符过滤器) -> tokenizer(分词器) -> TokenFilters(分词过滤器)[0个或多个] -> output 

        字符过滤:  使用字符过滤器转变字符
            通过分词 过滤器 & => and 将内容转换成  share experience with NoSql and big technologies

        文本切分为分词: 将文本切分为单个或多个分词
            将上一步的内容按标准分词器分词，分成一个一个的单词

        分词过滤: 使用分词过滤器转变每个分词
            分词过滤器可以是一个链，包含多个分词过滤:
                常用的有 小写 转换，停用词，同义词 等, 最后被分为： share experience with nosql  big technologies tools

        分词索引:  将分词存储到索引中       

        除了分词索引，上面三个步骤可以为一个定制分析器，该分析其可以由自己定制 

    可以在创建索引的时候，通过 analysis代码块设置具体的分析器。如下。该例子中就是在index索引下设置了该索引的analyzer(分析器)是ik分析器。es默认提供了 standard , english ,stop , lower等分析器，也可以自定义自己的分析器
        PUT /my_index
        {
            "settings": {
                "analysis": {
                    "char_filter": {
                        "&_to_and": {
                            "type":       "mapping",
                            "mappings": [ "&=> and "]
                    }},
                    "filter": {
                        "my_stopwords": {
                            "type":       "stop",
                            "stopwords": [ "the", "a" ]
                    }},
                    "analyzer": {
                        "my_analyzer": {
                            "type":         "custom",
                            "char_filter":  [ "html_strip", "&_to_and" ],
                            "tokenizer":    "standard",
                            "filter":       [ "lowercase", "my_stopwords" ]
                    }}
        }}}
    也可以在es的配置中设置分析器，在 elasticsearch.yml中配置

    使用_analyze 可以查看分词器将文本分成了什么样的内容
    GET /_analyze
    {
      "tokenizer": "standard",
      "filter": ["lowercase"],
      "text": ["hello Good Me"]
    }
    GET /_analyze
    {
      "tokenizer": "ik_smart",
      "text": ["我不知道的你是什么样子的，你爱我吗"]
    }

point11: 分析器、分词器、分词过滤器
    1.内置的分析器:( analyzer )
        标准分析器(standard,默认的文本分析器)，包括了 标准分词器，标准分词过滤器，小写转换分词过滤器和停用词分词过滤器
        简单分析器(simple) 只使用了小写转换分词器。意味着非字母出分词，分词转换成小写
        空白分析器(whitespace) 只是根据空白将文本分为若干个分词
        停用词分析器(stop) 和简单分析器行为很像，只是在分词流中额外的过滤了停用词
        关键词分析器(keyword) 将整个字段当作一个单独的分词。最好是将index设置为false，而不是在映射中使用关键词分析器
        模式分析器(pattern) 允许指定一个分词切分的模式
        语言和多语言分析器: 支持许多能直接使用的特定语言分析器，可以使用语言的名字来指定
        雪球分析器(snowball) 标准分词器和分词过滤器，还有小谢粉刺过滤器和停用词过滤器。还是用雪球词干器对文本进行词干提取
    2.内置的分词器 (tokenizer)
        标准分词器(standard) 基于语法的分词器，对于欧洲语言很不错。分词默认的最大长度是255
        关键词分词器(keyword) 将整个文本作为单个的分词，提供给分词过滤器。(如 想要分词 Hi,here,使用该分词器的结果还是 Hi,here)
        字母分词器(letter) 根据非字母的符号切分
        小写分词器(lowercase) 结合常规的字符分词器和小写分词过滤器(分词后，会将结果转换为小写)
        空白分词器(whitespace) 通过空白分词
        模式分词器(pattern) 指定任意的模式切分
        UAX，URL电子邮件分词器
        路径层次分词器(path hierarchy) 以特定的方式索引文件系统的路径。搜索时，共享同样路径的文件将被作为结果返回
    3.分词过滤器(filter)
        标准(standard) 基本上没做什么事情，就是去除单词结尾的 's 和不必要的句点字符，而且还是在很老版本的lucene中，现在已经被其他分词过滤器处理了
        小写(lowercase) 字符变小写
        长度(length) 将长度超出最短和最长限制范围的单词过滤掉
        停用词(stopword) 将停用词从分词流中移除  可以通过 analysis下的fitlter构建，可以指定几个字符串，也可以指定一个文件
        同义词(synonym) 找出分词的同义词
        截断(trunate) 默认截断多于10个字符的部分
        修剪(trim) 删除反此种的所有空白部分
        限制分词数量(limit token count) 限制某个字段可包含分词的最大数量。如果创建了一个定制的分词数量过滤器，限制是8，那么分词流中只有前8个分词会被索引 默认使用 max_token_count ，默认是1 ，只有一个分词会被索引

point12: 提取词干
    将单词缩减到基本或词根的形式 如 administrations ,其 词根 是 administr ，然用户匹配所有同样词根的单词。如 administrator,administration,administrate都是同一种词根
    包括 算法提取词干(使用 snowball、porter stem、kstem过滤器)，字典提取词干，或者重写分词过滤器提取词干

point13: N元语法过滤器



es 打分机制
详情可参考: https://www.cnblogs.com/jiangxinyang/p/10516302.html
point14: 评分
    词频(TF term frequency): 一个词条在该文档中出现的次数。如果 elasticsearch在文档1中出现的次数比在文档2中出现的次数多，那么文档2就比文档1的得分高

    逆文档频率(inverse document frequency):
        文档频率(df):一个单词在不同文档中出现的次数.(如果一个单词在一个文档中出现了3次，那他的文档频率也是1，该词频是3)
        逆文档频率是 1/df 。 所以 文档频率约高，逆文档频率越低，权重就越低

    评分公式:TF-IDF
        给定查询 q 和 文档 d ，其得分是 查询中每个词条 t 的 得分总和。而每个词条的得分是该词在文档d中的词频的平方根，乘以该词逆文档频率的平方和，乘以该文档字段的归一化因子，乘以该次的提升权重 
        score(query,document) = 
        词条的词频越高，得分越高; 索引中词条越罕见，逆文档频率越高。
        调和因子考虑了搜索过多少文档以及发现了多少词条。查询标准化是试图让不同查询的结果具有可比性
        默认的 TF-IDF 打分方法是 由 TF-IDF和向量空间模型的结合

    其他打分方法:(可以通过mapping修改某个字段映射中的similarity参数，在type下。还可以通过索引的setting设置similarity ，还可以在 es.yml中永久的配置索引使用其他的打分算法)
        BM25 : es 7 之后，使用该评分机制为默认评分机制。 一种基于 概率的打分框架
            bm25计算的分数可以认为是给定文档和查询匹配的概率。
            该算法以能更好地处理短字段而著称
            主要设置
                k: 默认1.2 控制对于得分而言 词频(tf) 的重要性
                b: 默认0.75。 介于0-1，控制了文档篇幅对于得分的影响程度
                discount_overlaps: 默认 true ， 告知es，某个字段中，多个分词出现在同一个位置，是否应该影响长度的标准化


boosting: 一个可以用来修改文档的相关性的程序，当索引或者查询文档的时候，可以提升一篇文档的的得分。索引期间修改文档的boosting是存储在索引中的，修改boosting的值需要重新索引文档。所以建议在查询的时候使用boosting，查询更灵活
    boosting的数值并不是精确的乘数。在计算分数得时候，boost数值是被标准化 得。

point15:索引期间得boost:
        在mapping的时候就通过properties设置好boost
            PUT mapping_test2/_mapping
            {
              "properties":{
                "field_7":{
                  "type" : "text",
                  "boost":2.0
                }
              }
            }
        不建议使用boost索引得原因是：1.索引得boost是固定得，如果要修改这个值，必须重新索引；2.boost值是以低精度得数值存储在lucene中，计算文档的最终得分可能丢失精度；3.boost是运用于词条得，boost字段如果匹配了多个词条，就意味着多次得boost，会进一步增加字段得权重
point16:查询期间的boost:
        使用基本的match,multi_match,simple_query_string或者query_string查询时，可以基于某个词条或者某个字段来控制boost.
        如下的 示例中，IndicatorDate 的boost大于 IndicatorArea,其最终的得分会比IndicatorArea拥有更大的影响力。
            GET bdms_index_380_summarydata/_search
            {
              "from": 0,
              "size": 200,
              "query": {
                "bool": {
                  "filter": [
                    {
                      "bool": {
                        "must": [
                          {
                            "term": {
                              "IndicatorDate": {
                                "value": "1483200000000",
                                "boost": 2
                              }
                            }
                          },
                          {
                            "term": {
                              "IndicatorArea": {
                                "value": "1483200000000",
                                "boost": 1
                              }
                            }
                          }
                        ],
                        "adjust_pure_negative": true,
                        "boost": 1
                      }
                    }
                  ]
                }
              }
            }
        针对multi_match这种可以查询多个字段的来说，可以使用 ^ 来使用boost
        GET bdms_index_462_acwf_hand_work_cooperative_manage/_search
        {
          "query": {
            "multi_match": {
              "query": "织金县",
              "fields": ["address","name^2"]
            }
          },
          "_source": {
            "includes": ["address","name"]
          }
        }


point17:使用explain来查看 具体的评分，分析匹配的规则，找出未能匹配的原因
    GET bdms_index_462_acwf_hand_work_cooperative_manage/_search
    {
      "query": {
        "multi_match": {
          "query": "织金县",
          "fields": ["address","name^2"]
        }
      },
      "_source": {
        "includes": ["address","name"]
      },
      "explain": true
    }
    explain的结果:
            "hits" : [
              {
                "_shard" : "[bdms_index_462_acwf_hand_work_cooperative_manage][3]",
                "_node" : "F2A5IO4VQUSUOGBiZ4Nykw",
                "_index" : "bdms_index_462_acwf_hand_work_cooperative_manage",
                "_type" : "_doc",
                "_id" : "K6sAM3sB-z8hYu2zavit",
                "_score" : 13.036671,
                "_source" : {
                  "address" : "毕节市织金县官寨乡大寨村",
                  "name" : "织金县民艺制品厂"
                },
                "_explanation" : {
                  "value" : 13.036671,      // 得分
                  "description" : "max of:", // 描述 这个描述代表着是求details里面的最大值
                  "details" : [
                    {
                      "value" : 6.984891,
                      "description" : "sum of:", // 求detais里面的总和
                      "details" : [
                            {
                              "value" : 2.328297,
                              "description" : "weight(address:织金县 in 16) [PerFieldSimilarity], result of:",
                              "details" : [
                                {
                                  "value" : 2.328297,
                                  "description" : "score(freq=1.0), computed as boost * idf * tf from:",// 计算分值 的公式，boost，idf，tf都由下面的details来
                                  "details" : [
                                    {
                                      "value" : 2.2,
                                      "description" : "boost",
                                      "details" : [ ]
                                    },

point18: function_score来定制得分
    function_score 查询 允许用户指定任何数量的任意函数，让他们作用于匹配了初始查询的文档，修改其得分，从而达到精细化控制结果相关性的目的。
    这种情况下，每种函数(function)是一个json小片段，以某种方式来影响得分
    function_score 的基本使用语法。functions里面的function会作用在match匹配上的文档之上
    GET bdms_index_462_acwf_hand_work_cooperative_manage/_search
    {
      "query": {
        "function_score": {
          "query": {
            "match": {
              "address": "织金县"
            }
          },
          "functions": [ // 空的function_list
           
          ]
        }
      }
    }
    集中function_score中的函数:
        1.weight函数: 将得分乘以一个常数。下面的例子时在address=织金县的基础上,name=企业的文档得分 * 2
            GET bdms_index_462_acwf_hand_work_cooperative_manage/_search
            {
              "query": {
                "function_score": {
                  "query": {
                    "match": {
                      "address": "织金县"
                    }
                  },
                  "functions": [
                      {
                        "weight": 2,
                        "filter": {"term": {
                          "name": "企业"
                        }}
                      }
                  ]
                }
              }
            }
        2.合并得分:
            针对函数的得分有两个:
            a.从每个单独的函数而来的得分是如何合并的，score_mode: 可以指定score_mode参数，有 multiply ,sum,avg,first,max,min 。默认 每个函数的得分是相乘的。
            b.得分合并:boost_mode.控制原始查询的得分和函数得分是如何合并的。如果没有指定，新的得分是初始查询得分和函数得分相乘。可设置为 sum,avg,max,min,replace(被函数得分替换)

    字段数据:当需要在某个字段上进行排序或者聚集操作时，需要es快速匹配出每个文档哪个词条用于排序或者聚合，这对于es的倒排索引来说支持的不是很好，这个时候就涉及到字段数据
        字段数据缓存是一种内存型的缓存。通常是在第一次需要使用时被构建，然后被保存用于不同的操作，如果是很多数据，那么第一次搜索会变得很慢。(预热器在这个时候就有了很大得用处)
            字段数据缓存得必要:因为许多比较、分析得操作都会处理大量数据，想要快速完成这些操作最好得方式就是分文内存中得数据，所以使用了缓存，将数据存放在java堆空间之中
        可以通过mapping设置 fielddata:true 来 设置字段 提前加载到缓存。不必等到第一次使用时才缓存。

        字段数据的使用主要有以下场景:
            按照某个字段进行排序，
            在某个字段上进行聚集
            在搜索请求中，使用fielddata_fields从字段数据来获取内容
            function_score中，使用field_value_factor、decay函数
            等
        其中最常见的就是 排序和聚集

        如何管理字段数据: 管理字段数据的目的是 避免集群中出现的问题，如 JVM垃圾回收花费太长时间，加载使用了过多内存导致OOM等。
        具体的管理方式:
            1.限制字段数据使用的内存量
                将字段数据缓存限制在一个固定的大小。可以设置内存使用量的大小来限制，也可以通过设置字段数据在缓存里失效的过期时间来限制。
                indices.fielddata.cache.size: 400mb  缓存大小 ，也可以设置 为 40% 百分比形式，让字段占用jvm堆的 40% 。 当设置了size之后，如果达到使用量的上限，将使用近期最少使用的策略(LRU)来淘汰数据
                indices.fielddata.cache.expire: 25m  过期时间
            2.使用字段数据的断路器
                断路器的作用是为了避免将过多的数据加载到内存中。监控加载到内存中的数据容量，如果达到一定的上限，就启动。默认限制为JVM虚拟机堆大小的60%
            3.使用文档值(doc value)来避免内存的使用。
                在创建索引mapping 的时候，通过setting设置字段的文档时，doc_values=true/false
                文档值 在文件被索引的时候，获取了将要加载到内存中的数据，并将他们和普通索引一起放到磁盘上。使用字段数据通常会使内存不够，而文档值可以从磁盘读取。优点如下:
                    性能平滑下降: 文档值从磁盘读取，和其他索引一样。如果
                    更高的内存管理:系统核心会将文档值缓存到内存中，避免了和堆使用相关的垃圾回收成本
                    更快的加载: 通过文档值，索引的阶段会计算非倒排结构，即使是首次运行查询，es没必要进行动态的正向话
                文档值的缺点:
                    更大的索引规模: 将所有的文档值存储在磁盘，索引变大
                    稍微变慢的索引过程: 索引阶段需要计算文档值，使得索引的过程变慢
                    使用字段数据的请求，会稍微变慢: 磁盘比内存读取速度慢
                    仅对非分析字段有效

聚集(aggregation):
    两个主要的类别:
        度量型(metrics):是指一组文档的统计分析，可以得到最小值、最大值、标准差等度量值

        桶型(bucket):将匹配的文档切分为一个或多个容器，告诉你每个桶里的文档书量
    聚集的通用写法:
        GET bdms_index_462_t_sp_gas_area_discharge/_search
        {
          "query": {                                    // 还可以在查询之后使用聚合
            "match": {
                "location": "Denver"
            }
          },
          "from": 0,
          "size": 0,                                    // 这两句将不展示hit命中的内容，不影响聚集的数量，聚集操作是在所有和查询相匹配的结果上执行的
          "aggregations": {
            "regionname": {                             // 聚合之后的名称，可以在请求的回复中看到这个名字
              "terms": {                                // 指定聚集类型的词条     ，必须要使用terms
                "field": "regionname.keyword",          // 使用字段的keyword形式，可以进行聚集操作
                "size": 200,
                "min_doc_count": 1,
                "shard_min_doc_count": 0,
                "show_term_doc_count_error": false,
                "order": [                              // 默认的排序
                  {
                    "_count": "desc"
                  },
                  {
                    "_key": "asc"
                  }
                ]
              },
              "aggregations": {                         // 还可以针对字段，进行聚集，此处是在select 中，查询的 count 的数量
                "count": {
                  "value_count": {
                    "field": "_index"
                  }
                }
              }
            }
          }
        }

集群扩展:
1.向集群中加入节点
    在同一个机器的不同目录或不同机器上启动　elasticsearch 启动脚本，就可以启动多个节点。当es 只有一个节点的时候，使用_cluster/health 查看, status是yellow，因为此时主节点已经分配，但是副本分片还没有。大于等于两个节点之后，status=green，此时尚未分配的副本分片就会分配到新的节点。大于一个节点之后，主要编号相同(同一份内容)的主分片和副本分片不在同一个节点上，那么并不禁止将主分片和副本分片放在同一个节点上。
2.发现其他es节点
    1. 广播
        当es启动的时候，它发送广播(multicast)的ping请求到地址224.2.2.4的54328端口，而其他的es节点使用同样的集群名称响应了这个请求。所以要尽量保证 es 的  cluster.name 的 具体性。尽量不要使用默认的名称，以免加入其他的集群

    2.单播
        让es连接一系列的主机，并试图发现更多关于集群的信息。当节点的ip地址不会经常变化，或者es的生产系统只连接特定的节点而不是整个网络的时候，单播是很理想的模式。
            通过elasticsearch.yml中设置 discovery.zen.ping.unicast.hosts:["",""]，来配置集群的单播地址。可以不必要将每个节点都配置所有的集群地址，满足口口相传的特点就行

    3.选举主节点
        一旦集群中的节点发现彼此，就会协商谁将成为主节点。主节点会负责管理集群的状态(当前的设置，集群中分片、索引、节点状态等)。主节点被选举出来后，会建立内部的ping机制来确保每个节点在集群种保持活跃和健康,其叫做错误识别(fault detection)
        
        es 默认所有节点都有资格成为主节点，除非某个节点的 node.master设置为false。
        选举:
          a.对所有可以成为master的节点根据nodeId排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第0位）节点，暂且认为它是master节点。
          b.如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举。
          c.对于brain split问题，需要把候选master节点最小值设置为可以成为master节点数n/2+1（quorum ）

        设置主节点的最小数量，可以告诉es 在集群成为健康状态前，集群中多少个节点有资格成为主节点。如果节点数量不会变化，那么将最小数量设置为 集群的总节点数。如果节点数量会变化。设置为总节点数 除以 2 再 加1.
        将minimum_master_nodes设置为高于1 的数量，可以预防集群产生脑裂的问题
            引申问题: 造成脑裂的原因:
                        1. 网络抖动 : 内网一般不会出现es集群的脑裂，外网出现脑裂的几率较大
                        2.节点负载 : 如果主节点同时承担数据节点的工作，可能因为工作负载大导致对应的es实例停止响应
                        3. 内存回收 : 由于数据节点上es 进程占用的内存较大，较大规模的内存回收也能造成es进程失去响应
                    如何尽量避免脑裂:
                        1.不要把主节点同时设为数据节点(node.master和node.data不要同时设置为true)
                        2.将节点响应超时(discovery.zen.ping_timeout)稍稍设置时间长一些(默认3s)，避免误判
                        3.设置需要超过半数的备选节点，才能发生主节点重选(discovery.zen.minimum_master_nodes = 半数以删备选主节点数)

    4.错误的识别
        当集群有两个节点(包括选举的主节点)之后，它需要和集群中的所有节点通信，以确保一切正常，这称为 错误识别的过程。主节点ping集群中所有其他的节点，而且每个节点也会ping主节点来确认无须选举。
        每个节点间隔 discovery.zen.fd.ping_interval(默认1s)时间发送一个ping请求，等待discovery.zen.fd.ping_timeout(默认30s)得时间，并尝试ping_retries(默认3次)次，然后宣布节点失联，并且在需要的时候进行新的节点路由和主节点选举。
3.删除节点
    删除节点后，集群的状态会变为黄色，这个时候，正常的节点会将掉线节点的主分片的副分片转化为主分片(因为 索引的的操作首先会更新主分片)，然后会针对丢失的副本分片重新在 集群中创建新的副本分片
4.停用节点
    如果想关闭某个节点，同时保持集群为绿色状态，可以先停用节点，这个操作会将待停用节点的所有分片转移到集群中的其他节点
    put _cluster/settings
    {
        "transient": {
            "cluster.routing.allocation.exclude._ip" : "xxx.xx.xx.xx"
        }
    }
5.升级es节点
    1.轮流重启: 轮流执行以下步骤进行每个节点的升级
        关闭集群的分配设置  cluster.routing.allocation.enable=none (由于直接关闭节点会进行副本的重新分配，但升级节点不需要重新分配，只需要将当前的节点转换为副本节点即可，所以需要关闭重新分配) 设置为 all开启分配
        关闭即将升级的节点
        升级节点
        启动升级后的节点
        等待升级后的节点加入集群
        开启集群的分配设置
        等待集群回复绿色的状态
    2.最小化重启后的回复时间
6.扩展策略:
  生产环境的es集群至少有三件事情需要考虑 : 过度分片，将数据切分为索引和分片，最大化吞吐量
  过度分片:
    es 默认对每个索引 都设置了5个分片，相当于创建一个索引时会将这个索引 分配成5份，同时每份都有一个对应的副本(如果是两个节点)。不设置为1 的好处是 如果数据量持续增多，当达到单个分片最大存储之后，需要重新进行索引的reindex才可以继续存储。但分片的个数也不能设置的过大，es管理每个分片都还有隐含的额外开销(包括占用的内存，压缩的比例等等)，所以需要找好对应的分片的个数。
  数据切分为索引和分片:

  最大化吞吐量:  该策略包含多种，可以包括 最大化索引的吞吐量，让搜索更快一些；一次执行多词搜索等；
    索引的时候，增加吞吐量可以采用将副本数量较少到1或者0，来增加吞吐量
    搜索的时候，可以采用增加更多副本来提高搜索的吞吐量

7. 别名
  别名 是你使用的指针或名称，对应于1个或多个具体的索引。
  1): 别名的用处: 当索引分片创建的不够，需要增加索引时，可以将 新创建的索引添加到别名对应的索引中，查询的时候使用对应别名；或者当有一组索引时按时间统计一周的数据的时候，也可以将这些索引统一放置到一个别名中
  2): 管理别名: 通过下面的语句对别名进行添加和删除
    POST _aliases
    {
      "actions": [
        {
          "add": {
            "index": "test1",
            "alias": "alias1"
          }
        },
        {
          "remove": {
            "index": "",
            "alias": ""
          }
        }
      ]
    }
8.路由:
  es散列文档时候，会对文档进行路由，来决定文档应该索引到哪个分片中，路由的规则可以由自己进行指定。
  查询的时候，可以手动的指定文档的路由，查询指定路由下的数据

9.性能提升:
  1) 合并请求，索引数据的时候，尽量保证批量索引，速度会更快，但还要控制批量索引的数量的大小，可能太大，占用过多的内存; 过小，网络开销会很大，所以需要自行测试，找到一个平衡点。
    搜索的请求也可以使用 批量查询的方式， url中使用 msearch调用接口，在代码中使用 MultiSearchRequest + client.msearch() 进行查询
  2) 优化lucene分段的处理
      es接收到应用发送的文档，会将其索引到内存中称为分段的倒排索引中，这些索引会不时的写入磁盘。这些分段不能改变，只能删除。同时小分段定时会合并成大分段。分段的优化方式主要分为以下三种:
        刷新(refresh)和冲刷(flush)
          刷新: es是一个近实时的搜索引擎，近实时主要体现在 默认的情况下，es每秒自动的刷新每份索引，所以es搜索的内容其实是一秒之前的内容。可以修改 index.refresh_interval 字段值改变索引的刷新间隔。当增加refresh_interval值时，将会获得更大的吞吐量，同时也可以将其设置为-1，关闭自动刷新，使用手动刷新
          冲刷: 刷新和冲刷得过程相互独立得。  数据首先索引到内存中，经过一次刷新后，es会搜索相应得内存片段，将内存中的分段提交到磁盘上的lucene索引过程，叫冲刷。
                为了保证某个节点宕机或分片以动位置时，内存数据不会丢失，es使用事务日志来跟踪尚未冲刷得索引操作。除了将内存分段提交到磁盘，冲刷还会清理事务日志。

              满足以下条件之一就会发生冲刷:
                内存缓冲区满 可以再 elasticsearch.yml中修改indices.memory.index_buffer_size的值
                自上次冲刷后超过了一段时间 index.translog.flush_threshold_period，针对某个索引修改 默认30分钟
                事务日志达到了一定得阈值 index.translog.flush_threshold_size ，针对某个索引修改
          对于分段，需要搜索的分段越多，搜索的速度越慢。

        合并的策略
          es中得分段 是不变的，很容易被缓存，搜索更快。修改数据集时，无需重建现有分段中的数据索引。但不好的是更新文档不能修改实际的文档，只能重新索引一个新文档。
          删除文档也不能直接从分段中移除(这样需要重建倒排索引)，需要单独的 .del文件 将其标记为已被删除，然后再合并分段的时候将其删除

          合并分段的意义: 1.将分段的总数量保持在受控的范围内(保障查询的性能)，2.真正的删除文档

          默认的合并策略是分层配置，如果你的分段多余某一层中所设置的最大分段数，该层的合并就会被触发

          合并发生在索引、更新或删除文档的时候，索引，合并的越多，这次操作成本越贵。想快速地索引，需要较少的合并，而且牺牲一些查询的性能

          分层合并的策略的流程
            1.冲刷操作在第一层中添加分段，知道存在过多分段，此处假设4个就太多了
            2.小的分段合并到较大的分段，而冲刷还在持续加入小分段
            3.最终较大的层也会拥有4个分段
            4.这四个较大的分段持续合并为一个更大的分段，上述过程不断持续
            5.直到分层达到了设置的限制值，之后，只有较小的分段被合并，最大分段保持不变
          合并策略的几个选项:
            index.merge.policy.segments_per_tier 该值越大，每层可以拥有的分段越多。意味着更少的合并以及更好的索引性能。如果索引次数不多，更好的搜索性能，该值设置的低一些;
            index.merge.policy.max_merge_at_once 限制了每次可以合并多少个分段，通常等同于 segments_per_tier 的值。需要确保 max_merge_at_once小于segments_per_tier ，确保不会引起过多的合并
            index.merge.policy.max_merged_segment 定义了最大的分段规模。不会再使用其他的分段合并为比这个更大的分段了
            index.merge.scheduler.max_thread_count 可用于后台合并的最大线程数。在多cpu和告诉I/O的机器上，可以增加这个设置，在低速CPU和IO上需要降低
          对于确定不会变更的索引，可以使用 优化 (强制性的合并) 来降低分段的数量，提高搜索的性能。
        存储和存储限流
          存储限流: es目前使用存储限流来限制合并可以使用的IO吞吐量(indices.store.throttle.max_bytes_per_sec),1.5版本默认20mb ,   如果有更为告诉的硬盘，需要更多的IO吞吐用于合并，可以增加存储限流的限制，也可以将indices.store.throttle.type设置为none，完全取消这个限制

          配置存储: es会将索引存储到数据目录，可以通过elasticsearch.yml的path.data属性来修改数据目录。默认的存储实现将索引文件存放到文件系统，为了访问lucene的分段文件，默认的存储实现使用了lucene的MMapDirectory，通常用于大型文件或者需要随机访问的文件。对于其他类型的文件，如存储字段，es使用了lucene的NIOFSDirectory。

          MMapDirectory 利用了文件缓存，请求操作系统将所需的文件映射到虚拟内存，这样能更快地直接访问内存。但并不是所有的文件都在内存中。如果内存规模大于可用的物理内存，操作系统会见没有使用的文件移出缓存，为需要读取的新文件腾出空间。若es再洗需要哪些未被缓存的文件，这些文件会再被加载到内存中。

          NIOFSDirectory NIOFSDirectory是直接访问文件的，但是它必须将所需的数据复制到jvm堆的缓存中，适合操作小型的、按顺序访问的文件。解决问题的描述(内存映射的文件，也会导致额外的负载，因为应用程序必须要告诉操作系统再访问文件之前先对其进行映射 )

          es默认的安装方式对文件的相关设置有些保守，可以手动更改相应 的变量，包括 MAX_OPEN_FILES=65535 ,MAX_MAP_COUNT=262144。还需要以启动es的用户运行
            ulimit -n 65535,以root身份为虚拟内存运行 sysctl -w vm.max_map_count = 262144
  3) 充分利用缓存
     1. 过滤器和过滤器缓存
        默认情况下，过滤器的查询结果是可以被缓存的，同时也可以指定_cache旗标来控制一个过滤器是否被缓存
      a.过滤器缓存
        过滤器的结果被缓存之后会存储在过滤器缓存中。默认值是10% ，可以通过更改indices.cache.filter.size 来更改其大小。当缓存满的时候，es移除最近最少使用的缓存(LRU),为新缓存内容留出空间，这样会发生回收操作。回收操作很多的缓存会使得性能下降，可以设置缓存条目的生存时间(TTL)，让缓存自动失效。
      b.组合过滤器
        当使用组合过滤器的使用，需要确保当过滤器组合之后，缓存得到良好的使用，而且过滤器是按照正确的顺序运行的
        位集合 是一个紧凑的位数组，es用它来缓存某个文档是否和过滤器匹配。常用的range过滤器，terms、term过滤器使用位集合进行缓存，script过滤器不使用位集合(遍历所有文档)
        位集合和普通的结果缓存的不同之处在于:
          位集合很紧凑而且很容易创建，所以再过滤器首次运行时创建缓存的额外开销不大
          他们是按照独立的过滤器来存储的，如，你在两个不同的查询中或者bool过滤器中使用一个terms过滤器，该term的位集合就可以重用
          可以很容易的和其他的位集合进行组合
        bool过滤器中组合了其他的位集合过滤器，同时会根据 must或者should进行位的and或or操作。而也可以在filter中直接使用and，or,not过滤器来组合多个过滤器。区别就是and、or、not这种过滤会先运行第一个过滤器，然后将匹配的文档传到下一个过滤器
        如果一个过滤条件中既包含位集合，也包含非位集合的过滤器，应该将位集合的过滤器放在前面，轻量级的过滤器应该在更消耗资源的过滤器之前
      c.在字段数据上运行过滤器
     2. 分片查询缓存
     3. JVM堆和操作系统缓存
      部署es的机器上，内存主要使用在连个位置， jvm堆的内存和 操作系统的缓存。如果jvm堆的内存过小，将导致将会导致频繁的垃圾回收，增加GC的时间，从而可能导致节点被剔除集群；如果jvm堆的内存设置过大(超过32G)，jvm会自动的使用未压缩指针，造成内存的浪费。而如果给系统缓存留的内存过小的话，将会导致索引的缓存无法正常使用
      经典的解决办法 是 遵循  一半 原则 ，将一般的内存分配给系统缓存(尽量保证热数据都在缓存中，提高查询效率)，将一般的内存分配个jvm(最好不要超过32G)
     4. 使用预热器让缓存热身
      可以在索引之上定义一个预热器(warmer),定义包括查询、过滤器、排序条件和聚集。一旦定义完成之后，预热器会让es在每次刷新操作之后，运行这个查询，这样做会使得刷新变慢，但是用户的查询将总是运行在 预热 后的缓存上
  4) 其他权衡条件
    a. 非精确匹配 : 非精确匹配可以使用 包括 模糊查询、前缀查询或过滤器 、 通配符查询 、 N元语法 在内的查询方式。 使用哪种查询方式需要根据考虑具体的情况来进行:
              模糊查询会拖慢查询，但索引和精确匹配一样，保持不变
              N元语法由于将单词的每个部分产生分词，导致索引规模可以增加数倍，同时 想修改N元语法的配置，需要重建全部的索引。但使用N元语法 搜索会 快一些 。

              N元语法 的使用条件 主要在于 低查询延迟或者需要很多的并发查询。每个查询消耗更少的cpu资源。其查询的条件就是需要更大的缓存或者更高效的磁盘读写
              普通的模糊查询则 使用在 较高的索引吞吐量，或者磁盘读写满。以及需要经常修改的数据。
    b. 脚本的使用 : 脚本 给查询提供了更多的灵活性，但是 要慎重使用脚本查询。因为脚本常常更耗费时间和cpu，而且脚本的结果不会被缓存。所以如果想使用脚本的话，需要深入脚本代码，并进行一定的性能优化。

    c. 权衡网络开销 : es 查询的方式 默认是 query_then_fetch , 即 先返回文档的ID以及用于排序的元数据给协调节点，协调节点排序后，再从分片中获取所需的前size篇文档的具体内容。可以减少多数情况下的网络开销，但由于会引发两次网络传输，所以是 then 。
                    
                    但query_then_fetch这种方式也会有一个问题。由于得分采用TF/IDF的方式，其中df 文档频率 展示的是 搜索词条在所有文档中出现了多少次，但这个所有文档代表的是这个分片上的所有文档，如果文档分布的不是那么均匀，就有可能出现某个节点上词频少，但权重更高的现象，导致结果出现问题。针对这种情况，可以采用 dfs_query_then_fetch , 这种方式 协调节点会向所有的分片发送一次额外的请求，来收集被搜词条的文档频率，然后再根据这个频率进行搜索。

                    还有一种查询方式是 query_and_fetch , 即直接从每个分片中取 size 个数的文档，然后集合到协调器节点，排序文档，取前十个返回。但是这样会增加网络的开销，同时由于所有的文档都集中在协调节点，还可能导致协调节点内存占用很大，导致协调节点出现问题。
    d. 分页查询的权衡: 当分页查询到后面几百页的时候，使用普通的查询就会变得很慢，增加内存得压力，可以使用scan或者 scoll 来进行滑动 得查询


管理集群
  1.模板: 对于同一种类型的索引，可以采用模板进行索引的一些通用配置, 下为模板的一个配置的json样例
        PUT _template/logging_index
        {
          "index_patterns": ["logstash-*"], 
          "settings": {
            "number_of_shards": 2, 
            "number_of_replicas": 1
          }
          , "mappings": {
            "properties": {}
          },
          "aliases": {
            "NAME": {}
          }
        }
        该命令会告诉es 当一个索引的名称 和 index_parrterns 中的模式匹配时，运用这个模板。但客户端向 es 发送一个心得事件，而指定得索引并不存在得时候，就会使用这个模板创建一个新的索引。该索引还可以设置别名，可以聚合某个指定月份内的全部索引。

        模板的配置除了通过url的方式，还可以通过在文件系统中定义 json文件的方式，更容易管理

        检索模板: get _template/logging_index 进行检索模板 也可以 _template/logging_index1,logging_index2检索多个模板

  2.集群的健康情况
    GET _cluster/health
    {
      "cluster_name" : "elasticsearch",
      "status" : "yellow",               // 集群状态 。绿色:主分片和副本分片都已经分发且运作正常;黄色:通常是副本分片丢失的信号;红色:危险，无法找到集群中的主分片，使得主分片上的索引操作不能进行，导致不一致的查询结果。
      "timed_out" : false,      
      "number_of_nodes" : 1,             // 节点总数量
      "number_of_data_nodes" : 1,        // 存放数据的节点数量
      "active_primary_shards" : 490,     // 集群中，全部索引的主分片总数量
      "active_shards" : 490,             // 集群中全部索引的所有分片，八廓主分片和副本分片的总数量
      "relocating_shards" : 0,           // 当下正在多个节点间移动的分片数量 ，该属性 >0 代表es正在集群内以动数据的分片，来提升负载均衡和故障转移。通常发生在添加新节点、重启失效节点、删除节点时候，是一种临时现象。
      "initializing_shards" : 0,         // 新创建的分片数量。 当用户刚刚创建一个新的索引或者重启一个节点的时候，该值大于0 .(那新加一个节点会不会也大于0)
      "unassigned_shards" : 487,         // 集群中定义的，却未能发现的分片数量 。 该值大于0 最常见的原因是有 尚未分配的副本分片， 
      "delayed_unassigned_shards" : 0,
      "number_of_pending_tasks" : 0,
      "number_of_in_flight_fetch" : 0,
      "task_max_waiting_in_queue_millis" : 0,
      "active_shards_percent_as_number" : 50.15353121801432
    }

    通过level参数，可以深入了解那些索引收到了分片未配置的影响
    GET _cluster/health?level=indices

  3.CPU , 慢日志，热线程和线程池

    1) 慢日志: 默认两者都是关闭的。日志输出是分片级别的。日志文件中，一个操作可能由若干行表示。日志的一些设置可以通过 {index_name}/settings 端点来修改
          慢查询日志:
          慢搜索日志:
        通过查看慢查询/搜索 日志，可以针对查询较慢的搜索进行调优，提高性能

  4.热线程API接口
    如果集群的CPU使用率居高不下，热线程(hot_threads)API可以用于识别被阻塞并导致问题的具体进程 。 热线程API提供了集群中每个节点上的一系列阻塞线程
    es 每过几个毫秒 就会收集每个线程的持续时间、状态(等待/阻塞)、等待持续时间 以及 阻塞持续时间等相关信息。过了指定的时间间隔(默认500毫秒)，es会对同样的信息进行第二轮收集操作，每次收集过程，会对每个堆栈轨迹拍摄快照。可以通过api请求调整参数，调整信息收集
        _nodes/hot_thread?type=wait&interval=1000ms&threads=3
        type: cpu 、 wait 、 block 之一，需要快照的线程状态类型
        interval: 第一次和第二次检查之间的等待时间，默认500毫秒
        thread: 排名靠前的 热 线程的展示数量

  es中存在两类过滤器缓存: 索引级别的过滤器缓存(不推荐，因为用户无法预测索引将会存放在进群中的何处，无法预计内存的使用量)，节点级别的过滤器缓存(默认，采用LRU缓存类型) 可以通过indices.cache.filter.size 设置缓存的大小

  字段数据缓存用于提升查询的执行时间，字段值会被加载到内存中并保存在字段数据的缓存中，用于之后的请求。默认地，这是一个没有限制的缓存，会持续增长，直到触动了字段数据的断路器，就会将数据从缓存结构中移除。

  断路器是人为 的限制，用于降低outofmemory异常出现的频率。es中由两个断路器，请求断路器和 父辈断路器(在所有断路器可能使用的内存总量上又设置了限制)
      indices.breaker.total.limit  默认堆内存的70%，不允许字段数据和请求断路器超越这个限制
      indices.breaker.fielddata.limit 默认堆内存的60%，不允许字段数据的缓存超越这个限制
      indices.breaker.request.limit 默认堆内存的40%，控制分配给聚集桶创建这种操作的对大小

备份数据:
  使用快照API堆数据进行备份。首次备份的时候，es将复制集群的状态和数据，后续的快照将包含前一个版本的之后的修改。快照的进程是非阻塞的，不会对运行的系统的性能产生明显的影响。
  快照存储在资料库之中，资料库可以定义为文件系统或者url
    文件系统: 需要是一个共享的文件系统，共享文件系统必须安装在集群的每个节点上 (本地系统、hdfs、amazon s3都可)
    url: url资料库是只读 的，可以作为替代的快照存储方案

  es使用snapshot API 的形式进行快照的备份和恢复
    首先，定义一个资料库
    PUT http:xxxx:9200/_snapshot/my_backup      // my_backup代表备份 资料库 的名字
    {
        "type": "fs",                           // 资料库的类型定义为共享的文件系统
        "settings": {
            "location": "/data/backup/my_backup", // 资料库的位置
            "max_snapshot_bytes_per_sec" : "50mb",// 快照进入仓库时，限流的速度
            "max_restore_bytes_per_sec" : "50mb"，// 从仓库恢复时，限流的速度
            "compress": true                      // 默认true，压缩元数据
        }
    }
    然后 , 查看资料库是否存在
    GET http:xxxx:9200/_snapshot/my_back
    然后 , 初始快照/备份
    PUT http:xxxx:9200/_snapshot/my_back/first_snapshot?wait_for_completion=true wait_for_completion=true代表快照运行结束才返回结果。不加则直接返回结果
    PUT _snapshot/my_backup/snapshot_2 // 使用该命令也可以之备份其中几个索引
    {
        "indices": "index_1,index_2"
    }
    GET _snapshot/my_backup/snapshot_1 查看某个快照

    DELETE _snapshot/my_backup/snapshot_1  删除快照，使用delete删除，不建议直接删除备份文件

集群恢复，只需要在希望恢复的 集群上运行即可

POST http:xxxx:9200/_snapshot/my_backup/snapshot_1/_restore  恢复某个快照
POST http:xxxx:9200/_snapshot/my_backup/snapshot_1/_restore?wait_for_completion=true 等待结果返回


控制es 分片和副本的分配

  如果有4台机器，a,b,c,d 想让索引 index1 分配在 a,b上， 索引 index2 分配在 c,d 上，index3分布在所有节点上，可以通过以下方式配置

  a,b节点 的 elasticsearch.yml 文件中，node.tag: zone_one
  b,d节点 的 elasticsearch.yml 文件中，node.tag: zone_two

  创建索引的时候，可以指定索引放到指定的空间中

  POST "http:/xxxx:9200/man'  先创建索引
  PUT "http://xxxx:9200/man/_settings' -d '{    设置索引的 分配节点的tag为 zone_one
   "index.routing.allocation.include.tag" : "zone_one"
   }'
  PUT "http://xxxx:9200/man/_settings' -d '{    设置索引不分配到节点的tag为 zone_two
   "index.routing.allocation.include.tag" : "zone_two"
   }'

  这种方式开可以做数据的冷热分离，将热数据放到一组服务器中，冷数据放到一组服务器中，通过定时将索引的 include.tag 修改为冷数据的tag，进行热数据到冷数据的转移，这个过程es会自动的转移

  数据的读写分离:
    该操作让读在特定的节点上，保证写节点的性能。该过程无法自动配置完成，需要通过reroute api来进行手动索引分片的分配。

    首先需要把 cluster.routing.allocation.disable_allocation参数设为true，禁止系统自动分配。然后使用api操作

    curl -XPOST 'localhost:9200/_cluster/reroute' -d '{  
    "commands" : [ {  
        "move" :    移动  把分片从一个节点移动到另一个节点，指定索引名和分片号
            {  
              "index" : "test", "shard" : 0,   
              "from_node" : "node1", "to_node" : "node2"  
            }  
        },  {
       "cancel" :   取消，取消node1节点上 正在分配的分片
            {  
              "index" : "test", "shard" : 0, "node" : "node1"  
            }  
        },  
        {  
          "allocate" : {   分配一个 未分配的分片 到指定节点 node3 上
              "index" : "test", "shard" : 1, "node" : "node3"  
          }  
        }  
    ]  
}' 


es的倒排索引:
https://ricstudio.top/archives/es-lucene-reverted-index